{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28270ea4",
   "metadata": {},
   "source": [
    "# PRODIGY RECIPES\n",
    "\n",
    "#### Idea: This notebook is to automate all the recipe generations including the paths and names of the datasets. \n",
    "\n",
    "#### Author: Vaishnavi Kandala\n",
    "\n",
    "#### Date: 7th Jan 2022\n",
    "\n",
    "#### Note: Each recipe components are rightly used before running the recipe. Ensure the right PORT is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6056e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "08c1da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/azureuser/vaishnavi/trends_&_innovation_classes.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ce03e07",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/azureuser/vaishnavi/trends_&_innovation_classes.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_z/ph_vfxq91rv80bcd6wnbnvnh0000gp/T/ipykernel_9877/3635080528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/azureuser/vaishnavi/trends_&_innovation_classes.tsv'"
     ]
    }
   ],
   "source": [
    "df_categories = pd.read_csv(path,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9944867e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Innovation/Trend</th>\n",
       "      <th>Specific Innovation/Trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Innovation</td>\n",
       "      <td>3D printed clothes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Innovation</td>\n",
       "      <td>3D printing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Innovation</td>\n",
       "      <td>Autonomous transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Innovation</td>\n",
       "      <td>Biking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Innovation</td>\n",
       "      <td>Capsule wardrobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Innovation/Trend Specific Innovation/Trend\n",
       "0         Innovation        3D printed clothes\n",
       "1         Innovation               3D printing\n",
       "2         Innovation      Autonomous transport\n",
       "3         Innovation                    Biking\n",
       "4         Innovation          Capsule wardrobe\n",
       "..               ...                       ...\n",
       "994              NaN                       NaN\n",
       "995              NaN                       NaN\n",
       "996              NaN                       NaN\n",
       "997              NaN                       NaN\n",
       "998              NaN                       NaN\n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e0d7649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_name_processing(category_name):\n",
    "    category_name = category_name.lower()\n",
    "    category_name = re.sub(\" \",\"_\",category_name)\n",
    "    return category_name\n",
    "\n",
    "category_name_processing(\"3D printed clothes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d86bfe",
   "metadata": {},
   "source": [
    "## General CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ccb3486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_name = \"Artificial Intelligence\"\n",
    "category_name_processed = category_name_processing(category_name)\n",
    "log_path = \"/home/azureuser/christine/logs/\"\n",
    "patterns_path = \"/home/azureuser/datadrive/code/christine/patterns/\"\n",
    "raw_data_path = \"/home/azureuser/christine/data/\"\n",
    "model_path = \"/home/azureuser/christine/model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f20615",
   "metadata": {},
   "source": [
    "PORTS ASSIGNED\n",
    "\n",
    "PORT = 8084 path = \n",
    "PORT = 8086 path. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ef456",
   "metadata": {},
   "source": [
    "### RECIPE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22173c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 1 - RECIPE 1\n",
    "\n",
    "\n",
    "# Recipe 1 - This is mainly to get suggestions on the keywords relevant to a particular category. We are using a pre-trained model to get these suggestions (since, our categories are generic). We think we have good suggestions with this model but it can be further explored.\n",
    "\n",
    "# PRODIGY_PORT=8084 PRODIGY_CONFIG_OVERRIDES='{\"validate\": false}' nohup prodigy sense2vec.teach AI_Seed_Words model/s2v_old --seeds \"artificial intelligence, machine learning, natural language processing, supervised learning, deep learning\" > AIseedsOut.out 2>&1 &\n",
    "\n",
    "# Description:\n",
    "\n",
    "# PRODIGY_PORT - please use the specific port assigned to your task.\n",
    "# PRODIGY_CONFIG_OVERRIDES - to override any generic settings in the configuration. Here we do validate - false to ensure no input is missed because of formats.\n",
    "# sense2vec.tech - is the prodigy model or recipe\n",
    "# AI_Seed_Words - name of the dataset\n",
    "# models/s2v_old - is the selected model to generate embeddings.\n",
    "# seeds - seed words for the category\n",
    "# output location - AISeedsOut.out is the name of the output (log) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6cb5ec76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PRODIGY_PORT=8084 nohup prodigy sense2vec.teach artificial_intelligence_keywords model/s2v_old --seeds 'artificial intelligence, machine learning, natural language processing, supervised learning, deep learning' > /home/azureuser/christine/logs/artificial_intelligence_keywords.out 2>&1 &\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port_value_task1 = str(8084)\n",
    "task1_name = category_name_processed + \"_keywords\"\n",
    "seed_words = \"'artificial intelligence, machine learning, natural language processing, supervised learning, deep learning'\"\n",
    "task1_log = log_path + task1_name + \".out\"\n",
    "\n",
    "### RECIPE Generation\n",
    "\n",
    "\"PRODIGY_PORT=\" + port_value_task1  + \\\n",
    "\" nohup prodigy sense2vec.teach \" + task1_name + \\\n",
    "\" model/s2v_old --seeds \" + seed_words + \" > \" + task1_log + \" 2>&1 &\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72879ed1",
   "metadata": {},
   "source": [
    "#### GENERATE PATTERNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b78f441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prodigy terms.to-patterns artificial_intelligence_keywords /home/azureuser/datadrive/code/christine/patterns/artificial_intelligence_keywords.jsonl --label artificial_intelligence'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### CONVERT SEED WORDS TO PATTERNS\n",
    "task1_patterns = patterns_path + task1_name + \".jsonl\"\n",
    "\n",
    "\"prodigy terms.to-patterns \" +  task1_name + \" \" + task1_patterns + \" --label \" + category_name_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b83829",
   "metadata": {},
   "source": [
    "### RECIPE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf0ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recipe 2 - This is mainly to train an initial model by selecting the sentences with the keywords (generated in the previous recipe). The more exhaustive keywords are a better model can be trained.\n",
    "\n",
    "#PRODIGY_PORT=8084 nohup prodigy textcat.teach AI_Words en_core_web_sm prodigyData/AI_TrainingData.jsonl --label \"Artificial Intelligence\" --patterns prodigyData/AI_Seed_Word_Patterns.jsonl > AIwordsOut.out 2>&1 &\n",
    "\n",
    "# Description:\n",
    "\n",
    "# PRODIGY_PORT - use the specific port for the task.\n",
    "# textcat.teach - prodigy recipe to train a base model\n",
    "# AI_Words - the name of the dataset\n",
    "# en_core_web_sm - model for embeddings\n",
    "# AI_TrainingData.jsonl - training data formatted in JSONL format (use the GitHub function to generate prodigy formatted dataset).\n",
    "# label - label for this call.\n",
    "# patterns - keywords generated using recipe 1 to further select specific category based sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa02260a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRODIGY_PORT=8089 nohup prodigy textcat.teach artificial_intelligence_sentences en_core_web_sm /home/azureuser/christine/data/artificial_intelligence_raw_data.jsonl --label artificial_intelligence --patterns /home/azureuser/datadrive/code/christine/patterns/artificial_intelligence_keywords.jsonl > /home/azureuser/christine/logs/artificial_intelligence_sentences.out 2>&1 &'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port_value_task2 = str(8089)\n",
    "task2_name = category_name_processed + \"_sentences\"\n",
    "task2_data_path = raw_data_path + category_name_processed + \"_raw_data.jsonl\"\n",
    "task2_log = log_path + task2_name + \".out\"\n",
    "\n",
    "\"PRODIGY_PORT=\" + port_value_task2  + \\\n",
    "\" nohup prodigy textcat.teach \" +  task2_name + \\\n",
    "\" en_core_web_sm \"+ task2_data_path + \\\n",
    "\" --label \" + category_name_processed + \\\n",
    "\" --patterns \" + task1_patterns + \\\n",
    "\" > \" + task2_log + \" 2>&1 &\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4e130",
   "metadata": {},
   "source": [
    "### RECIPE 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recipe 3 - This is mainly to retrain the model for that category using the annotations gathered using recipe 2.\n",
    "\n",
    "#PRODIGY_PORT=8084 prodigy train --textcat-multilabel AI_Words --label-stats --verbose --eval-split 0.2 model/AI-model\n",
    "\n",
    "# Description:\n",
    "\n",
    "# PRODIGY_PORT - use specific port for the task.\n",
    "# textcat_multilabel - retraining the model with the annotation data.\n",
    "# AI_Words - dataset used for annotations in recipe 2.\n",
    "# eval_split - 80% train 20% test.\n",
    "# AI-model - model used.\n",
    "# This recipe will give the performance Precision / Recall / F-Score for the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a517b9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRODIGY_PORT=8089 nohup prodigy train --textcat-multilabel artificial_intelligence_sentences --label-stats --verbose --eval-split 0.2 /home/azureuser/christine/model/artificial_intelligence_model'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port_value_task2 = str(8089)\n",
    "task3_model_path = model_path + category_name_processed + \"_model\"\n",
    "\n",
    "\"PRODIGY_PORT=\" + port_value_task2  + \\\n",
    "\" nohup prodigy train --textcat-multilabel \" + \\\n",
    "task2_name + \" --label-stats --verbose --eval-split 0.2 \" + \\\n",
    "task3_model_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
